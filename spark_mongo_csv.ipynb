{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04ce7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c9549e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/20 10:00:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "working_directory = '/home/spark/Downloads/rapids-4-spark_2.12-23.02.0.jar,/home/spark/spark/jars/mongo-spark-connector/bson-4.8.2.jar,/home/spark/spark/jars/mongo-spark-connector/bson-record-codec-4.8.2.jar,/home/spark/spark/jars/mongo-spark-connector/mongodb-driver-core-4.8.2.jar,/home/spark/spark/jars/mongo-spark-connector/mongodb-driver-sync-4.8.2.jar,/home/spark/spark/jars/mongo-spark-connector/mongo-spark-connector_2.12-10.1.1.jar'\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .master(\"local[3]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2G\") \\\n",
    "    .config(\"spark.executor.memory\", \"2G\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri=mongodb://10.122.6.17:27017/vnw_job.events\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri=mongodb://10.122.6.17:27017/vnw_job.events\") \\\n",
    "    .config(\"spark.jars\", \"/home/spark/Downloads/rapids-4-spark_2.12-23.02.0.jar,/home/spark/spark/jars/mongo-spark-connector/bson-4.8.2.jar,/home/spark/spark/jars/mongo-spark-connector/bson-record-codec-4.8.2.jar,/home/spark/spark/jars/mongo-spark-connector/mongodb-driver-core-4.8.2.jar,/home/spark/spark/jars/mongo-spark-connector/mongodb-driver-sync-4.8.2.jar,/home/spark/spark/jars/mongo-spark-connector/mongo-spark-connector_2.12-10.1.1.jar,/home/spark/Downloads/spark-tensorflow-connector_2.11-1.14.0.jar\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29fb9ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a789c069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''MySQL'''\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "# database = \"vnw_core\"\n",
    "database = 'in_inter'\n",
    "host = \"172.16.1.75\"\n",
    "\n",
    "cnx = mysql.connector.connect(\n",
    "    host=host,\n",
    "    port=\"3306\",\n",
    "    user='ylv', \n",
    "    password=\"b693DTDoud\", \n",
    "    database=database\n",
    ")\n",
    "\n",
    "query = \"\"\"select * from in_inter.tblresume_industry_sample_20230705 where checker=1;\"\"\"\n",
    "\n",
    "cursor = cnx.cursor()\n",
    "cursor.execute(query)\n",
    "data = cursor.fetchall()\n",
    "df_sql = pd.DataFrame(data, columns=[i[0] for i in cursor.description])\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89c60a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resumeid</th>\n",
       "      <th>userid</th>\n",
       "      <th>resumetitle</th>\n",
       "      <th>desiredjobtitle</th>\n",
       "      <th>mostrecentposition</th>\n",
       "      <th>mostrecentemployer</th>\n",
       "      <th>industryOld</th>\n",
       "      <th>industryV3Name</th>\n",
       "      <th>checker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>266457</td>\n",
       "      <td>815561</td>\n",
       "      <td>2022-03-15 05:59:46 Sang Tôn Thất</td>\n",
       "      <td>MEP Project Manager</td>\n",
       "      <td>MEP Project Manager</td>\n",
       "      <td>Foreigner Company</td>\n",
       "      <td>Executive management</td>\n",
       "      <td>Real Estate/Rental/Leasing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>484328</td>\n",
       "      <td>417254</td>\n",
       "      <td>2019/04/04 00:30:23 Phi Hùng Huỳnh</td>\n",
       "      <td>Chuyên viên kinh doanh - Dịch vụ</td>\n",
       "      <td>Customer Service/ Sales/ Marketing</td>\n",
       "      <td>From 2005  to now, working for some companies.</td>\n",
       "      <td>Customer Service,Sales,Sales Technical</td>\n",
       "      <td>Automotive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>538689</td>\n",
       "      <td>1155840</td>\n",
       "      <td>2023-07-06 06:59:47 Thị Duy Linh Bùi</td>\n",
       "      <td>Phiên dịch tự do</td>\n",
       "      <td>Phiên dịch tự do</td>\n",
       "      <td>Phiên dịch tự do</td>\n",
       "      <td>Administrative/Clerical,Sales,Interpreter/Tran...</td>\n",
       "      <td>Media/Newspaper/Advertising</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1711433</td>\n",
       "      <td>2412350</td>\n",
       "      <td>2023-04-11 22:41:45 KIEN NGUYEN</td>\n",
       "      <td>Procurement Manager</td>\n",
       "      <td>Procurement Manager</td>\n",
       "      <td>Nova Service Group</td>\n",
       "      <td>Merchandising/Purchasing/Supply Chain</td>\n",
       "      <td>FMCG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1835176</td>\n",
       "      <td>1257373</td>\n",
       "      <td>2023-05-24 20:21:33 Đạt Võ</td>\n",
       "      <td>Senior Sales Executive</td>\n",
       "      <td>Senior Sales Executive</td>\n",
       "      <td>Fujitec Vietnam Co., Ltd.</td>\n",
       "      <td>Sales,Sales Technical</td>\n",
       "      <td>Electrical/Electronics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>10497849</td>\n",
       "      <td>7286334</td>\n",
       "      <td>2023-07-06 08:35:29 Quế Anh  Lưu Dương</td>\n",
       "      <td>Sinh Viên</td>\n",
       "      <td>Tình nguyện viên</td>\n",
       "      <td>LIFT charitable organization</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>IT Software/SaaS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>10497896</td>\n",
       "      <td>7285717</td>\n",
       "      <td>CV_Lê Thị Thảo Sương.pdf-2023-07-06</td>\n",
       "      <td>Phó phòng kế hoạch</td>\n",
       "      <td>Phó phòng Kế hoạch</td>\n",
       "      <td>Công ty Trách nhiệm hữu hạn một thành viên Ô t...</td>\n",
       "      <td>Other</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>10497903</td>\n",
       "      <td>7285793</td>\n",
       "      <td>CURRICULUM VITAE.docx-2023-07-06</td>\n",
       "      <td>Trưởng phòng kinh doanh</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>10497921</td>\n",
       "      <td>7286336</td>\n",
       "      <td>kỹ sư an toàn</td>\n",
       "      <td>kỹ sư an toàn</td>\n",
       "      <td>None</td>\n",
       "      <td>công ty NOVALAND</td>\n",
       "      <td>HSE</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>10497975</td>\n",
       "      <td>4356853</td>\n",
       "      <td>Sale Supervisor</td>\n",
       "      <td>Sale Supervisor</td>\n",
       "      <td>None</td>\n",
       "      <td>Trung Nam EMS</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>Labor Supply</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>359 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     resumeid   userid                             resumetitle  \\\n",
       "0      266457   815561       2022-03-15 05:59:46 Sang Tôn Thất   \n",
       "1      484328   417254      2019/04/04 00:30:23 Phi Hùng Huỳnh   \n",
       "2      538689  1155840    2023-07-06 06:59:47 Thị Duy Linh Bùi   \n",
       "3     1711433  2412350         2023-04-11 22:41:45 KIEN NGUYEN   \n",
       "4     1835176  1257373              2023-05-24 20:21:33 Đạt Võ   \n",
       "..        ...      ...                                     ...   \n",
       "354  10497849  7286334  2023-07-06 08:35:29 Quế Anh  Lưu Dương   \n",
       "355  10497896  7285717     CV_Lê Thị Thảo Sương.pdf-2023-07-06   \n",
       "356  10497903  7285793        CURRICULUM VITAE.docx-2023-07-06   \n",
       "357  10497921  7286336                           kỹ sư an toàn   \n",
       "358  10497975  4356853                         Sale Supervisor   \n",
       "\n",
       "                      desiredjobtitle                   mostrecentposition  \\\n",
       "0                 MEP Project Manager                  MEP Project Manager   \n",
       "1    Chuyên viên kinh doanh - Dịch vụ  Customer Service/ Sales/ Marketing    \n",
       "2                    Phiên dịch tự do                     Phiên dịch tự do   \n",
       "3                 Procurement Manager                  Procurement Manager   \n",
       "4              Senior Sales Executive               Senior Sales Executive   \n",
       "..                                ...                                  ...   \n",
       "354                         Sinh Viên                    Tình nguyện viên    \n",
       "355                Phó phòng kế hoạch                   Phó phòng Kế hoạch   \n",
       "356           Trưởng phòng kinh doanh                                 None   \n",
       "357                     kỹ sư an toàn                                 None   \n",
       "358                   Sale Supervisor                                 None   \n",
       "\n",
       "                                    mostrecentemployer  \\\n",
       "0                                    Foreigner Company   \n",
       "1       From 2005  to now, working for some companies.   \n",
       "2                                     Phiên dịch tự do   \n",
       "3                                   Nova Service Group   \n",
       "4                            Fujitec Vietnam Co., Ltd.   \n",
       "..                                                 ...   \n",
       "354                      LIFT charitable organization    \n",
       "355  Công ty Trách nhiệm hữu hạn một thành viên Ô t...   \n",
       "356                                               None   \n",
       "357                                   công ty NOVALAND   \n",
       "358                                      Trung Nam EMS   \n",
       "\n",
       "                                           industryOld  \\\n",
       "0                                 Executive management   \n",
       "1               Customer Service,Sales,Sales Technical   \n",
       "2    Administrative/Clerical,Sales,Interpreter/Tran...   \n",
       "3                Merchandising/Purchasing/Supply Chain   \n",
       "4                                Sales,Sales Technical   \n",
       "..                                                 ...   \n",
       "354                                        Entry level   \n",
       "355                                              Other   \n",
       "356                                              Sales   \n",
       "357                                                HSE   \n",
       "358                                   Customer Service   \n",
       "\n",
       "                  industryV3Name checker  \n",
       "0     Real Estate/Rental/Leasing       1  \n",
       "1                     Automotive       1  \n",
       "2    Media/Newspaper/Advertising       1  \n",
       "3                           FMCG       1  \n",
       "4         Electrical/Electronics       1  \n",
       "..                           ...     ...  \n",
       "354             IT Software/SaaS       1  \n",
       "355                Manufacturing       1  \n",
       "356                Manufacturing       1  \n",
       "357                Manufacturing       1  \n",
       "358                 Labor Supply       1  \n",
       "\n",
       "[359 rows x 9 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5478f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(map(str,df_sql.userid.unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59179db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "con = MongoClient(\"mongodb://10.122.6.17:27017\")\n",
    "cur = con.vnw_job.events.find({'entityId': {'$in': users}}, {\"entityId\": 1, \"targetEntityId\": 1, \"event\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26d17c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98eec7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./mapping_industries/resume_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1327140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"entityId\", StringType(), True),\n",
    "#     StructField(\"targetEntityId\", StringType(), True)\n",
    "# ])\n",
    "\n",
    "# df = spark.read.format(\"csv\").load(\"/home/spark/ylv/data/navidata/*\", schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a08a900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|            entityId|targetEntityId|\n",
      "+--------------------+--------------+\n",
      "|             6612804|       1624701|\n",
      "|52.9.73.55, 10.12...|       1619921|\n",
      "|             5132497|       1626036|\n",
      "|             7086582|       1629851|\n",
      "|             5291876|       1620867|\n",
      "|             1842295|       1621413|\n",
      "|68.169.42.157, 10...|       1630422|\n",
      "|             7075271|       1630145|\n",
      "|fdb8:b0b8:7f0:0:c...|       1630342|\n",
      "|             4764495|       1627604|\n",
      "+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f860ea45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:========================================>                 (9 + 4) / 13]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10040693"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "399b01ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.23 ms, sys: 1.15 ms, total: 2.38 ms\n",
      "Wall time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "# df = spark.read.format(\"mongodb\").option(\"database\", \"vnw_job\").option(\"collection\", \"events\").option(\"pipeline\", '[{\"$match\": {\"eventTime\": {\"$gte\": ISODate(\"2023-04-01T17:00:00Z\"), \"$lte\": ISODate(\"2023-04-02T17:00:00Z\")}}}]').load()\n",
    "# df = spark.read.format(\"mongodb\").option(\"database\", \"data_6\").option(\"collection\", \"events\").load()\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"entityId\", StringType(), True),\n",
    "    StructField(\"targetEntityId\", StringType(), True),\n",
    "    StructField(\"event\", StringType(), True)\n",
    "])\n",
    "\n",
    "%time df = spark.read.format(\"mongodb\").option(\"spark.mongodb.database\", \"vnw_job\").option(\"spark.mongodb.collection\", \"events\").option(\"aggregation.pipeline\", '[{\"$match\": {\"eventTime\": {\"$gte\": ISODate(\"2024-03-01T00:00:00Z\"), \"$lt\": ISODate(\"2024-04-01T00:00:00Z\")}}}, {\"$project\": {\"entityId\": 1, \"targetEntityId\": 1, \"event\": 1}}]').load(schema=schema)\n",
    "\n",
    "# %time df = spark.read.format(\"mongodb\").option(\"spark.mongodb.database\", \"vnw_job\").option(\"spark.mongodb.collection\", \"events\").option(\"aggregation.pipeline\", '[{\"$match\": {\"eventTime\": {\"$gte\": ISODate(\"2023-06-01T00:00:00Z\")}}}, {\"$project\": {\"entityId\": 1, \"targetEntityId\": 1, \"event\": 1}}]').load(schema=schema)\n",
    "# %time df = spark.read.format(\"mongodb\").option(\"spark.mongodb.database\", \"vnw_job\").option(\"spark.mongodb.collection\", \"events\").option(\"aggregation.pipeline\", '[{\"$match\": {\"eventTime\": {\"$gte\": ISODate(\"2023-07-01T00:00:00Z\"))}}}, {\"$project\": {\"entityId\": 1, \"targetEntityId\": 1, \"event\": 1}}]').load(schema=schema)\n",
    "# %time df = spark.read.format(\"mongodb\").option(\"spark.mongodb.database\", \"vnw_job\").option(\"spark.mongodb.collection\", \"events\").option(\"aggregation.pipeline\", '[{\"$match\": {\"entityId\": {\"$in\": [\"815561\", \"417254\"], \"$lt\": ISODate(\"2023-01-01T00:00:00Z\")}}}, {\"$project\": {\"entityId\": 1, \"targetEntityId\": 1, \"event\": 1}}]').load(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15f4eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8395562"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc154eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format('parquet').save('/home/spark/ylv/workplace/mongo_data/032024.parquet', schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0df162ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 15:31:51.951589: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/spark/miniconda3/envs/recommend/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_1699614/4133561072.py:16: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-whitegrid')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49bb3d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:================================================>        (11 + 2) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.41 s, sys: 51.5 ms, total: 1.46 s\n",
      "Wall time: 5.54 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "unique_userids = df.select(\"entityId\").distinct().collect()\n",
    "unique_itemids = df.select(\"targetEntityId\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8369886",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 15:32:40.685238: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-04-11 15:32:40.685264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 172.18.3.41\n",
      "2023-04-11 15:32:40.685269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 172.18.3.41\n",
      "2023-04-11 15:32:40.685325: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.39.1\n",
      "2023-04-11 15:32:40.685341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.39.1\n",
      "2023-04-11 15:32:40.685345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 510.39.1\n",
      "2023-04-11 15:32:40.685625: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "movies = tf.data.Dataset.from_tensor_slices({\n",
    "    \"job_id\": unique_itemids\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad5d83db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# tensor_slices = {\n",
    "#     \"user_id\": df.select(\"entityId\").collect(),\n",
    "#     \"job_id\": df.select(\"targetEntityId\")\n",
    "# }\n",
    "userids = df.select(\"entityId\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bec0105",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices({\"userid\": userids[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0d8d76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userid': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'6612804'], dtype=object)>}\n"
     ]
    }
   ],
   "source": [
    "for i in data.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14784016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "itemids = df.select(\"targetEntityId\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6811c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 14:36:06.822522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-11 14:36:06.847491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-11 14:36:06.847682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-11 14:36:06.848143: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-11 14:36:06.848851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-11 14:36:06.848989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-11 14:36:06.849114: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-11 14:36:07.268564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-11 14:36:07.268720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-11 14:36:07.268839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-11 14:36:07.268944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6326 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:02:00.0, compute capability: 7.5\n",
      "2023-04-11 14:36:07.270224: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 240976632 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# userids = df.select(\"targetEnityId\").collect()\n",
    "data = tf.data.Dataset.from_tensor_slices(tensor_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = tf.data.Dataset.from_tensor_slices(tensor_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "630a8798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 15:56:54 INFO V2ScanRelationPushDown: \n",
      "Output: entityId#0, targetEntityId#1\n",
      "         \n",
      "23/04/10 15:56:54 INFO Partitioner: Getting collection stats for: vnw_job.events\n",
      "23/04/10 15:56:54 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:56:54 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:56:54 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2307869, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:56:54 ICT 2023, lastUpdateTimeNanos=540508914963200}\n",
      "23/04/10 15:57:00 INFO Partitioner: Getting collection stats for: vnw_job.events\n",
      "23/04/10 15:57:33 INFO Partitioner: Getting collection stats for: vnw_job.events\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:05 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "23/04/10 15:58:05 INFO DAGScheduler: Got job 10 (save at NativeMethodAccessorImpl.java:0) with 36 output partitions\n",
      "23/04/10 15:58:05 INFO DAGScheduler: Final stage: ResultStage 12 (save at NativeMethodAccessorImpl.java:0)\n",
      "23/04/10 15:58:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/04/10 15:58:05 INFO DAGScheduler: Missing parents: List()\n",
      "23/04/10 15:58:05 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[37] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/04/10 15:58:05 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 221.8 KiB, free 433.9 MiB)\n",
      "23/04/10 15:58:05 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 80.2 KiB, free 433.8 MiB)\n",
      "23/04/10 15:58:05 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:36465 (size: 80.2 KiB, free: 434.2 MiB)\n",
      "23/04/10 15:58:05 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1478\n",
      "23/04/10 15:58:05 INFO DAGScheduler: Submitting 36 missing tasks from ResultStage 12 (MapPartitionsRDD[37] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "23/04/10 15:58:05 INFO TaskSchedulerImpl: Adding task set 12.0 with 36 tasks resource profile 0\n",
      "23/04/10 15:58:05 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 304) (localhost, executor driver, partition 0, ANY, 5005 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:58:05 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 305) (localhost, executor driver, partition 1, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:58:05 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 306) (localhost, executor driver, partition 2, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:58:05 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 307) (localhost, executor driver, partition 3, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:58:05 INFO TaskSetManager: Starting task 4.0 in stage 12.0 (TID 308) (localhost, executor driver, partition 4, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:58:05 INFO TaskSetManager: Starting task 5.0 in stage 12.0 (TID 309) (localhost, executor driver, partition 5, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:58:05 INFO TaskSetManager: Starting task 6.0 in stage 12.0 (TID 310) (localhost, executor driver, partition 6, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:58:05 INFO TaskSetManager: Starting task 7.0 in stage 12.0 (TID 311) (localhost, executor driver, partition 7, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:58:05 INFO TaskSetManager: Starting task 8.0 in stage 12.0 (TID 312) (localhost, executor driver, partition 8, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:58:05 INFO TaskSetManager: Starting task 9.0 in stage 12.0 (TID 313) (localhost, executor driver, partition 9, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:58:05 INFO TaskSetManager: Starting task 10.0 in stage 12.0 (TID 314) (localhost, executor driver, partition 10, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:58:05 INFO TaskSetManager: Starting task 11.0 in stage 12.0 (TID 315) (localhost, executor driver, partition 11, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:58:05 INFO Executor: Running task 1.0 in stage 12.0 (TID 305)\n",
      "23/04/10 15:58:05 INFO Executor: Running task 0.0 in stage 12.0 (TID 304)\n",
      "23/04/10 15:58:05 INFO Executor: Running task 2.0 in stage 12.0 (TID 306)\n",
      "23/04/10 15:58:05 INFO Executor: Running task 4.0 in stage 12.0 (TID 308)\n",
      "23/04/10 15:58:05 INFO Executor: Running task 5.0 in stage 12.0 (TID 309)\n",
      "23/04/10 15:58:05 INFO Executor: Running task 3.0 in stage 12.0 (TID 307)\n",
      "23/04/10 15:58:05 INFO Executor: Running task 7.0 in stage 12.0 (TID 311)\n",
      "23/04/10 15:58:05 INFO Executor: Running task 6.0 in stage 12.0 (TID 310)\n",
      "23/04/10 15:58:05 INFO Executor: Running task 8.0 in stage 12.0 (TID 312)\n",
      "23/04/10 15:58:05 INFO Executor: Running task 9.0 in stage 12.0 (TID 313)\n",
      "23/04/10 15:58:05 INFO Executor: Running task 10.0 in stage 12.0 (TID 314)\n",
      "23/04/10 15:58:05 INFO Executor: Running task 11.0 in stage 12.0 (TID 315)\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:05 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:58:05 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:05 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:58:05 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:05 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:58:05 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:58:05 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:58:05 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:58:05 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:58:05 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:58:05 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:58:05 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:58:05 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:58:05 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:58:05 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:58:05 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:58:05 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:58:05 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:58:05 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:58:05 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:58:05 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:58:05 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:58:05 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:58:05 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:58:05 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:58:05 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3961752, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:58:05 ICT 2023, lastUpdateTimeNanos=540579841417768}\n",
      "23/04/10 15:58:05 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4201688, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:58:05 ICT 2023, lastUpdateTimeNanos=540579841916620}\n",
      "23/04/10 15:58:05 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=5078112, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:58:05 ICT 2023, lastUpdateTimeNanos=540579842902929}\n",
      "23/04/10 15:58:05 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=5287207, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:58:05 ICT 2023, lastUpdateTimeNanos=540579844474825}\n",
      "23/04/10 15:58:05 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=6330958, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:58:05 ICT 2023, lastUpdateTimeNanos=540579845736648}\n",
      "23/04/10 15:58:05 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=6977911, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:58:05 ICT 2023, lastUpdateTimeNanos=540579846554048}\n",
      "23/04/10 15:58:05 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=7202606, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:58:05 ICT 2023, lastUpdateTimeNanos=540579846893988}\n",
      "23/04/10 15:58:05 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=7391536, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:58:05 ICT 2023, lastUpdateTimeNanos=540579849312545}\n",
      "23/04/10 15:58:05 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=6764269, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:58:05 ICT 2023, lastUpdateTimeNanos=540579849681143}\n",
      "23/04/10 15:58:05 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=7496162, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:58:05 ICT 2023, lastUpdateTimeNanos=540579850485761}\n",
      "23/04/10 15:58:05 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=7914856, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:58:05 ICT 2023, lastUpdateTimeNanos=540579851655919}\n",
      "23/04/10 15:58:05 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=6240891, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:58:05 ICT 2023, lastUpdateTimeNanos=540579853474512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 15:58:33 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:36465 in memory (size: 80.2 KiB, free: 434.3 MiB)\n",
      "23/04/10 15:58:42 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558056548402006485400041_0012_m_000011_315' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558056548402006485400041_0012_m_000011\n",
      "23/04/10 15:58:42 INFO SparkHadoopMapRedUtil: attempt_202304101558056548402006485400041_0012_m_000011_315: Committed\n",
      "23/04/10 15:58:42 INFO Executor: Finished task 11.0 in stage 12.0 (TID 315). 2515 bytes result sent to driver\n",
      "23/04/10 15:58:42 INFO TaskSetManager: Starting task 12.0 in stage 12.0 (TID 316) (localhost, executor driver, partition 12, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:58:42 INFO Executor: Running task 12.0 in stage 12.0 (TID 316)\n",
      "23/04/10 15:58:42 INFO TaskSetManager: Finished task 11.0 in stage 12.0 (TID 315) in 37702 ms on localhost (executor driver) (1/36)\n",
      "23/04/10 15:58:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:42 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:58:42 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:58:43 INFO FileOutputCommitter: Saved output of task 'attempt_20230410155805988721319113148994_0012_m_000007_311' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_20230410155805988721319113148994_0012_m_000007\n",
      "23/04/10 15:58:43 INFO SparkHadoopMapRedUtil: attempt_20230410155805988721319113148994_0012_m_000007_311: Committed\n",
      "23/04/10 15:58:43 INFO Executor: Finished task 7.0 in stage 12.0 (TID 311). 2515 bytes result sent to driver\n",
      "23/04/10 15:58:43 INFO TaskSetManager: Starting task 13.0 in stage 12.0 (TID 317) (localhost, executor driver, partition 13, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:58:43 INFO Executor: Running task 13.0 in stage 12.0 (TID 317)\n",
      "23/04/10 15:58:43 INFO TaskSetManager: Finished task 7.0 in stage 12.0 (TID 311) in 38153 ms on localhost (executor driver) (2/36)\n",
      "23/04/10 15:58:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:43 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:58:43 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:58:45 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=6099481, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:58:44 ICT 2023, lastUpdateTimeNanos=540619760647370}\n",
      "23/04/10 15:58:45 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=14364972, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:58:45 ICT 2023, lastUpdateTimeNanos=540620251948611}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 15:58:58 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558055528174118103338732_0012_m_000004_308' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558055528174118103338732_0012_m_000004\n",
      "23/04/10 15:58:58 INFO SparkHadoopMapRedUtil: attempt_202304101558055528174118103338732_0012_m_000004_308: Committed\n",
      "23/04/10 15:58:58 INFO Executor: Finished task 4.0 in stage 12.0 (TID 308). 2515 bytes result sent to driver\n",
      "23/04/10 15:58:58 INFO TaskSetManager: Starting task 14.0 in stage 12.0 (TID 318) (localhost, executor driver, partition 14, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:58:58 INFO Executor: Running task 14.0 in stage 12.0 (TID 318)\n",
      "23/04/10 15:58:58 INFO TaskSetManager: Finished task 4.0 in stage 12.0 (TID 308) in 53647 ms on localhost (executor driver) (3/36)\n",
      "23/04/10 15:58:58 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:58 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:58 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:58 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:58:58 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:58:59 INFO FileOutputCommitter: Saved output of task 'attempt_20230410155805154634324262293126_0012_m_000000_304' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_20230410155805154634324262293126_0012_m_000000\n",
      "23/04/10 15:58:59 INFO SparkHadoopMapRedUtil: attempt_20230410155805154634324262293126_0012_m_000000_304: Committed\n",
      "23/04/10 15:58:59 INFO Executor: Finished task 0.0 in stage 12.0 (TID 304). 2515 bytes result sent to driver\n",
      "23/04/10 15:58:59 INFO TaskSetManager: Starting task 15.0 in stage 12.0 (TID 319) (localhost, executor driver, partition 15, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:58:59 INFO Executor: Running task 15.0 in stage 12.0 (TID 319)\n",
      "23/04/10 15:58:59 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 304) in 54159 ms on localhost (executor driver) (4/36)\n",
      "23/04/10 15:58:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:58:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:58:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:58:59 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:58:59 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:58:59 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2733411, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:58:59 ICT 2023, lastUpdateTimeNanos=540633989151830}\n",
      "23/04/10 15:59:01 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=10707121, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:59:01 ICT 2023, lastUpdateTimeNanos=540635854632748}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 15:59:13 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558056421208899476067275_0012_m_000010_314' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558056421208899476067275_0012_m_000010\n",
      "23/04/10 15:59:13 INFO SparkHadoopMapRedUtil: attempt_202304101558056421208899476067275_0012_m_000010_314: Committed\n",
      "23/04/10 15:59:13 INFO Executor: Finished task 10.0 in stage 12.0 (TID 314). 2515 bytes result sent to driver\n",
      "23/04/10 15:59:13 INFO TaskSetManager: Starting task 16.0 in stage 12.0 (TID 320) (localhost, executor driver, partition 16, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:59:13 INFO Executor: Running task 16.0 in stage 12.0 (TID 320)\n",
      "23/04/10 15:59:13 INFO TaskSetManager: Finished task 10.0 in stage 12.0 (TID 314) in 68267 ms on localhost (executor driver) (5/36)\n",
      "23/04/10 15:59:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:59:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:59:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:59:13 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:59:13 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:59:13 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=9128913, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:59:13 ICT 2023, lastUpdateTimeNanos=540648127966948}\n",
      "23/04/10 15:59:20 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558051031612608560338728_0012_m_000008_312' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558051031612608560338728_0012_m_000008\n",
      "23/04/10 15:59:20 INFO SparkHadoopMapRedUtil: attempt_202304101558051031612608560338728_0012_m_000008_312: Committed\n",
      "23/04/10 15:59:20 INFO Executor: Finished task 8.0 in stage 12.0 (TID 312). 2515 bytes result sent to driver\n",
      "23/04/10 15:59:20 INFO TaskSetManager: Starting task 17.0 in stage 12.0 (TID 321) (localhost, executor driver, partition 17, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:59:20 INFO Executor: Running task 17.0 in stage 12.0 (TID 321)\n",
      "23/04/10 15:59:20 INFO TaskSetManager: Finished task 8.0 in stage 12.0 (TID 312) in 74891 ms on localhost (executor driver) (6/36)\n",
      "23/04/10 15:59:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:59:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:59:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:59:20 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:59:20 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:59:20 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=10340497, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:59:19 ICT 2023, lastUpdateTimeNanos=540654737113728}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 15:59:26 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558055846412203832210945_0012_m_000005_309' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558055846412203832210945_0012_m_000005\n",
      "23/04/10 15:59:26 INFO SparkHadoopMapRedUtil: attempt_202304101558055846412203832210945_0012_m_000005_309: Committed\n",
      "23/04/10 15:59:26 INFO Executor: Finished task 5.0 in stage 12.0 (TID 309). 2515 bytes result sent to driver\n",
      "23/04/10 15:59:26 INFO TaskSetManager: Starting task 18.0 in stage 12.0 (TID 322) (localhost, executor driver, partition 18, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:59:26 INFO Executor: Running task 18.0 in stage 12.0 (TID 322)\n",
      "23/04/10 15:59:26 INFO TaskSetManager: Finished task 5.0 in stage 12.0 (TID 309) in 80940 ms on localhost (executor driver) (7/36)\n",
      "23/04/10 15:59:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:59:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:59:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:59:26 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:59:26 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:59:27 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3930682, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:59:26 ICT 2023, lastUpdateTimeNanos=540661779257386}\n",
      "23/04/10 15:59:29 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558058772956942426441514_0012_m_000009_313' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558058772956942426441514_0012_m_000009\n",
      "23/04/10 15:59:29 INFO SparkHadoopMapRedUtil: attempt_202304101558058772956942426441514_0012_m_000009_313: Committed\n",
      "23/04/10 15:59:29 INFO Executor: Finished task 9.0 in stage 12.0 (TID 313). 2515 bytes result sent to driver\n",
      "23/04/10 15:59:29 INFO TaskSetManager: Starting task 19.0 in stage 12.0 (TID 323) (localhost, executor driver, partition 19, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:59:29 INFO Executor: Running task 19.0 in stage 12.0 (TID 323)\n",
      "23/04/10 15:59:29 INFO TaskSetManager: Finished task 9.0 in stage 12.0 (TID 313) in 84821 ms on localhost (executor driver) (8/36)\n",
      "23/04/10 15:59:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:59:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:59:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:59:30 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:59:30 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:59:30 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3111769, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:59:29 ICT 2023, lastUpdateTimeNanos=540664687368283}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 15:59:32 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558054891091164998782756_0012_m_000001_305' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558054891091164998782756_0012_m_000001\n",
      "23/04/10 15:59:32 INFO SparkHadoopMapRedUtil: attempt_202304101558054891091164998782756_0012_m_000001_305: Committed\n",
      "23/04/10 15:59:32 INFO Executor: Finished task 1.0 in stage 12.0 (TID 305). 2515 bytes result sent to driver\n",
      "23/04/10 15:59:32 INFO TaskSetManager: Starting task 20.0 in stage 12.0 (TID 324) (localhost, executor driver, partition 20, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:59:32 INFO Executor: Running task 20.0 in stage 12.0 (TID 324)\n",
      "23/04/10 15:59:32 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 305) in 87039 ms on localhost (executor driver) (9/36)\n",
      "23/04/10 15:59:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:59:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:59:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:59:32 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:59:32 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:59:32 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=221065379, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:59:32 ICT 2023, lastUpdateTimeNanos=540667101375356}\n",
      "23/04/10 15:59:33 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558053379132866718174597_0012_m_000012_316' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558053379132866718174597_0012_m_000012\n",
      "23/04/10 15:59:33 INFO SparkHadoopMapRedUtil: attempt_202304101558053379132866718174597_0012_m_000012_316: Committed\n",
      "23/04/10 15:59:33 INFO Executor: Finished task 12.0 in stage 12.0 (TID 316). 2515 bytes result sent to driver\n",
      "23/04/10 15:59:33 INFO TaskSetManager: Starting task 21.0 in stage 12.0 (TID 325) (localhost, executor driver, partition 21, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:59:33 INFO Executor: Running task 21.0 in stage 12.0 (TID 325)\n",
      "23/04/10 15:59:33 INFO TaskSetManager: Finished task 12.0 in stage 12.0 (TID 316) in 50616 ms on localhost (executor driver) (10/36)\n",
      "23/04/10 15:59:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:59:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:59:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:59:33 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:59:33 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:59:33 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=32795229, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:59:33 ICT 2023, lastUpdateTimeNanos=540668193555118}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 15:59:35 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558052481922418354184483_0012_m_000006_310' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558052481922418354184483_0012_m_000006\n",
      "23/04/10 15:59:35 INFO SparkHadoopMapRedUtil: attempt_202304101558052481922418354184483_0012_m_000006_310: Committed\n",
      "23/04/10 15:59:35 INFO Executor: Finished task 6.0 in stage 12.0 (TID 310). 2515 bytes result sent to driver\n",
      "23/04/10 15:59:35 INFO TaskSetManager: Starting task 22.0 in stage 12.0 (TID 326) (localhost, executor driver, partition 22, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:59:35 INFO Executor: Running task 22.0 in stage 12.0 (TID 326)\n",
      "23/04/10 15:59:35 INFO TaskSetManager: Finished task 6.0 in stage 12.0 (TID 310) in 90580 ms on localhost (executor driver) (11/36)\n",
      "23/04/10 15:59:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:59:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:59:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:59:35 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:59:35 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:59:36 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=6113360, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:59:36 ICT 2023, lastUpdateTimeNanos=540671434470052}\n",
      "23/04/10 15:59:42 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558054609442371467837193_0012_m_000002_306' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558054609442371467837193_0012_m_000002\n",
      "23/04/10 15:59:42 INFO SparkHadoopMapRedUtil: attempt_202304101558054609442371467837193_0012_m_000002_306: Committed\n",
      "23/04/10 15:59:42 INFO Executor: Finished task 2.0 in stage 12.0 (TID 306). 2515 bytes result sent to driver\n",
      "23/04/10 15:59:42 INFO TaskSetManager: Starting task 23.0 in stage 12.0 (TID 327) (localhost, executor driver, partition 23, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:59:42 INFO Executor: Running task 23.0 in stage 12.0 (TID 327)\n",
      "23/04/10 15:59:42 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 306) in 96931 ms on localhost (executor driver) (12/36)\n",
      "23/04/10 15:59:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:59:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:59:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:59:42 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:59:42 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:59:42 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3963011, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:59:41 ICT 2023, lastUpdateTimeNanos=540676770508144}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 15:59:57 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558054970433157473005132_0012_m_000003_307' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558054970433157473005132_0012_m_000003\n",
      "23/04/10 15:59:57 INFO SparkHadoopMapRedUtil: attempt_202304101558054970433157473005132_0012_m_000003_307: Committed\n",
      "23/04/10 15:59:57 INFO Executor: Finished task 3.0 in stage 12.0 (TID 307). 2515 bytes result sent to driver\n",
      "23/04/10 15:59:57 INFO TaskSetManager: Starting task 24.0 in stage 12.0 (TID 328) (localhost, executor driver, partition 24, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 15:59:57 INFO Executor: Running task 24.0 in stage 12.0 (TID 328)\n",
      "23/04/10 15:59:57 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 307) in 111922 ms on localhost (executor driver) (13/36)\n",
      "23/04/10 15:59:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 15:59:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 15:59:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 15:59:57 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 15:59:57 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 15:59:57 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3345406, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 15:59:56 ICT 2023, lastUpdateTimeNanos=540691831982927}\n",
      "23/04/10 16:00:13 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558058293806766194895831_0012_m_000022_326' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558058293806766194895831_0012_m_000022\n",
      "23/04/10 16:00:13 INFO SparkHadoopMapRedUtil: attempt_202304101558058293806766194895831_0012_m_000022_326: Committed\n",
      "23/04/10 16:00:13 INFO Executor: Finished task 22.0 in stage 12.0 (TID 326). 2515 bytes result sent to driver\n",
      "23/04/10 16:00:13 INFO TaskSetManager: Starting task 25.0 in stage 12.0 (TID 329) (localhost, executor driver, partition 25, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 16:00:13 INFO Executor: Running task 25.0 in stage 12.0 (TID 329)\n",
      "23/04/10 16:00:13 INFO TaskSetManager: Finished task 22.0 in stage 12.0 (TID 326) in 37964 ms on localhost (executor driver) (14/36)\n",
      "23/04/10 16:00:13 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 16:00:13 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 16:00:13 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 16:00:13 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 16:00:13 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 16:00:13 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=213493320, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 16:00:13 ICT 2023, lastUpdateTimeNanos=540708590804735}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 16:00:17 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558055248447893918576980_0012_m_000013_317' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558055248447893918576980_0012_m_000013\n",
      "23/04/10 16:00:17 INFO SparkHadoopMapRedUtil: attempt_202304101558055248447893918576980_0012_m_000013_317: Committed\n",
      "23/04/10 16:00:17 INFO Executor: Finished task 13.0 in stage 12.0 (TID 317). 2515 bytes result sent to driver\n",
      "23/04/10 16:00:17 INFO TaskSetManager: Starting task 26.0 in stage 12.0 (TID 330) (localhost, executor driver, partition 26, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 16:00:17 INFO Executor: Running task 26.0 in stage 12.0 (TID 330)\n",
      "23/04/10 16:00:17 INFO TaskSetManager: Finished task 13.0 in stage 12.0 (TID 317) in 93841 ms on localhost (executor driver) (15/36)\n",
      "23/04/10 16:00:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 16:00:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 16:00:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 16:00:17 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 16:00:17 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 16:00:17 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2843239, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 16:00:16 ICT 2023, lastUpdateTimeNanos=540711824637039}\n",
      "23/04/10 16:00:37 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558058932526410127275309_0012_m_000020_324' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558058932526410127275309_0012_m_000020\n",
      "23/04/10 16:00:37 INFO SparkHadoopMapRedUtil: attempt_202304101558058932526410127275309_0012_m_000020_324: Committed\n",
      "23/04/10 16:00:37 INFO Executor: Finished task 20.0 in stage 12.0 (TID 324). 2515 bytes result sent to driver\n",
      "23/04/10 16:00:37 INFO TaskSetManager: Starting task 27.0 in stage 12.0 (TID 331) (localhost, executor driver, partition 27, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 16:00:37 INFO Executor: Running task 27.0 in stage 12.0 (TID 331)\n",
      "23/04/10 16:00:37 INFO TaskSetManager: Finished task 20.0 in stage 12.0 (TID 324) in 65137 ms on localhost (executor driver) (16/36)\n",
      "23/04/10 16:00:37 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 16:00:37 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 16:00:37 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 16:00:37 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 16:00:37 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 16:00:37 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2816173, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 16:00:37 ICT 2023, lastUpdateTimeNanos=540732007293881}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 16:00:45 INFO FileOutputCommitter: Saved output of task 'attempt_20230410155805398690714149351162_0012_m_000015_319' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_20230410155805398690714149351162_0012_m_000015\n",
      "23/04/10 16:00:45 INFO SparkHadoopMapRedUtil: attempt_20230410155805398690714149351162_0012_m_000015_319: Committed\n",
      "23/04/10 16:00:45 INFO Executor: Finished task 15.0 in stage 12.0 (TID 319). 2515 bytes result sent to driver\n",
      "23/04/10 16:00:45 INFO TaskSetManager: Starting task 28.0 in stage 12.0 (TID 332) (localhost, executor driver, partition 28, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 16:00:45 INFO Executor: Running task 28.0 in stage 12.0 (TID 332)\n",
      "23/04/10 16:00:45 INFO TaskSetManager: Finished task 15.0 in stage 12.0 (TID 319) in 105722 ms on localhost (executor driver) (17/36)\n",
      "23/04/10 16:00:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 16:00:45 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 16:00:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 16:00:45 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 16:00:45 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 16:00:45 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2966917, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 16:00:44 ICT 2023, lastUpdateTimeNanos=540739711738940}\n",
      "23/04/10 16:00:54 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558053335169012425425935_0012_m_000016_320' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558053335169012425425935_0012_m_000016\n",
      "23/04/10 16:00:54 INFO SparkHadoopMapRedUtil: attempt_202304101558053335169012425425935_0012_m_000016_320: Committed\n",
      "23/04/10 16:00:54 INFO Executor: Finished task 16.0 in stage 12.0 (TID 320). 2515 bytes result sent to driver\n",
      "23/04/10 16:00:54 INFO TaskSetManager: Starting task 29.0 in stage 12.0 (TID 333) (localhost, executor driver, partition 29, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 16:00:54 INFO Executor: Running task 29.0 in stage 12.0 (TID 333)\n",
      "23/04/10 16:00:54 INFO TaskSetManager: Finished task 16.0 in stage 12.0 (TID 320) in 101064 ms on localhost (executor driver) (18/36)\n",
      "23/04/10 16:00:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 16:00:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 16:00:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 16:00:54 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 16:00:54 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 16:00:54 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=8071957, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 16:00:54 ICT 2023, lastUpdateTimeNanos=540749170934638}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 16:00:57 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558059220934181039852711_0012_m_000024_328' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558059220934181039852711_0012_m_000024\n",
      "23/04/10 16:00:57 INFO SparkHadoopMapRedUtil: attempt_202304101558059220934181039852711_0012_m_000024_328: Committed\n",
      "23/04/10 16:00:57 INFO Executor: Finished task 24.0 in stage 12.0 (TID 328). 2515 bytes result sent to driver\n",
      "23/04/10 16:00:57 INFO TaskSetManager: Starting task 30.0 in stage 12.0 (TID 334) (localhost, executor driver, partition 30, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 16:00:57 INFO Executor: Running task 30.0 in stage 12.0 (TID 334)\n",
      "23/04/10 16:00:57 INFO TaskSetManager: Finished task 24.0 in stage 12.0 (TID 328) in 60344 ms on localhost (executor driver) (19/36)\n",
      "23/04/10 16:00:57 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 16:00:57 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 16:00:57 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 16:00:57 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 16:00:57 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 16:00:57 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2826775, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 16:00:57 ICT 2023, lastUpdateTimeNanos=540752096432772}\n",
      "23/04/10 16:01:00 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558055197762454410360003_0012_m_000017_321' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558055197762454410360003_0012_m_000017\n",
      "23/04/10 16:01:00 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558053768733679262061437_0012_m_000014_318' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558053768733679262061437_0012_m_000014\n",
      "23/04/10 16:01:00 INFO SparkHadoopMapRedUtil: attempt_202304101558055197762454410360003_0012_m_000017_321: Committed\n",
      "23/04/10 16:01:00 INFO SparkHadoopMapRedUtil: attempt_202304101558053768733679262061437_0012_m_000014_318: Committed\n",
      "23/04/10 16:01:00 INFO Executor: Finished task 14.0 in stage 12.0 (TID 318). 2515 bytes result sent to driver\n",
      "23/04/10 16:01:00 INFO Executor: Finished task 17.0 in stage 12.0 (TID 321). 2515 bytes result sent to driver\n",
      "23/04/10 16:01:00 INFO TaskSetManager: Starting task 31.0 in stage 12.0 (TID 335) (localhost, executor driver, partition 31, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 16:01:00 INFO Executor: Running task 31.0 in stage 12.0 (TID 335)\n",
      "23/04/10 16:01:00 INFO TaskSetManager: Starting task 32.0 in stage 12.0 (TID 336) (localhost, executor driver, partition 32, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 16:01:00 INFO TaskSetManager: Finished task 14.0 in stage 12.0 (TID 318) in 122170 ms on localhost (executor driver) (20/36)\n",
      "23/04/10 16:01:00 INFO Executor: Running task 32.0 in stage 12.0 (TID 336)\n",
      "23/04/10 16:01:00 INFO TaskSetManager: Finished task 17.0 in stage 12.0 (TID 321) in 100925 ms on localhost (executor driver) (21/36)\n",
      "23/04/10 16:01:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 16:01:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 16:01:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 16:01:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 16:01:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 16:01:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 16:01:00 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 16:01:00 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 16:01:00 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 16:01:00 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 16:01:01 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4573368, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 16:01:00 ICT 2023, lastUpdateTimeNanos=540755650959101}\n",
      "23/04/10 16:01:01 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4595667, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 16:01:00 ICT 2023, lastUpdateTimeNanos=540755651162964}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 16:01:15 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558051166772161375833056_0012_m_000025_329' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558051166772161375833056_0012_m_000025\n",
      "23/04/10 16:01:15 INFO SparkHadoopMapRedUtil: attempt_202304101558051166772161375833056_0012_m_000025_329: Committed\n",
      "23/04/10 16:01:15 INFO Executor: Finished task 25.0 in stage 12.0 (TID 329). 2515 bytes result sent to driver\n",
      "23/04/10 16:01:15 INFO TaskSetManager: Starting task 33.0 in stage 12.0 (TID 337) (localhost, executor driver, partition 33, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 16:01:15 INFO Executor: Running task 33.0 in stage 12.0 (TID 337)\n",
      "23/04/10 16:01:15 INFO TaskSetManager: Finished task 25.0 in stage 12.0 (TID 329) in 61494 ms on localhost (executor driver) (22/36)\n",
      "23/04/10 16:01:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 16:01:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 16:01:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 16:01:15 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 16:01:15 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 16:01:15 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2997361, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 16:01:15 ICT 2023, lastUpdateTimeNanos=540769868196155}\n",
      "23/04/10 16:01:17 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558058216036825527341185_0012_m_000027_331' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558058216036825527341185_0012_m_000027\n",
      "23/04/10 16:01:17 INFO SparkHadoopMapRedUtil: attempt_202304101558058216036825527341185_0012_m_000027_331: Committed\n",
      "23/04/10 16:01:17 INFO Executor: Finished task 27.0 in stage 12.0 (TID 331). 2515 bytes result sent to driver\n",
      "23/04/10 16:01:17 INFO TaskSetManager: Starting task 34.0 in stage 12.0 (TID 338) (localhost, executor driver, partition 34, ANY, 5023 bytes) taskResourceAssignments Map()\n",
      "23/04/10 16:01:17 INFO Executor: Running task 34.0 in stage 12.0 (TID 338)\n",
      "23/04/10 16:01:17 INFO TaskSetManager: Finished task 27.0 in stage 12.0 (TID 331) in 39968 ms on localhost (executor driver) (23/36)\n",
      "23/04/10 16:01:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 16:01:17 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 16:01:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 16:01:17 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 16:01:17 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 16:01:18 INFO FileOutputCommitter: Saved output of task 'attempt_20230410155805530956130393717925_0012_m_000018_322' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_20230410155805530956130393717925_0012_m_000018\n",
      "23/04/10 16:01:18 INFO SparkHadoopMapRedUtil: attempt_20230410155805530956130393717925_0012_m_000018_322: Committed\n",
      "23/04/10 16:01:18 INFO Executor: Finished task 18.0 in stage 12.0 (TID 322). 2515 bytes result sent to driver\n",
      "23/04/10 16:01:18 INFO TaskSetManager: Starting task 35.0 in stage 12.0 (TID 339) (localhost, executor driver, partition 35, ANY, 5006 bytes) taskResourceAssignments Map()\n",
      "23/04/10 16:01:18 INFO Executor: Running task 35.0 in stage 12.0 (TID 339)\n",
      "23/04/10 16:01:18 INFO TaskSetManager: Finished task 18.0 in stage 12.0 (TID 322) in 112067 ms on localhost (executor driver) (24/36)\n",
      "23/04/10 16:01:18 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "23/04/10 16:01:18 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "23/04/10 16:01:18 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "23/04/10 16:01:18 INFO client: MongoClient with metadata {\"driver\": {\"name\": \"mongo-java-driver|sync|mongo-spark-connector|source\", \"version\": \"4.8.2|10.1.1\"}, \"os\": {\"type\": \"Linux\", \"name\": \"Linux\", \"architecture\": \"amd64\", \"version\": \"5.15.0-60-generic\"}, \"platform\": \"Java/Ubuntu/11.0.17+8-post-Ubuntu-1ubuntu220.04|Scala/2.12.15/Spark/3.2.3\"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@11729bda]}, clusterSettings={hosts=[10.122.6.17:27017], srvServiceName=mongodb, mode=SINGLE, requiredClusterType=UNKNOWN, requiredReplicaSetName='null', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=null}, applicationName='null', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null}\n",
      "23/04/10 16:01:18 INFO cluster: Cluster description not yet available. Waiting for 30000 ms before timing out\n",
      "23/04/10 16:01:18 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=8422417, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 16:01:18 ICT 2023, lastUpdateTimeNanos=540772848504573}\n",
      "23/04/10 16:01:18 INFO cluster: Monitor thread successfully connected to server with description ServerDescription{address=10.122.6.17:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=9, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3492757, setName='vnw', canonicalAddress=vnhcmvt-smartnavi04:27017, hosts=[vnhcmvt-smartnavi04:27017, vnhcmvt-smartnavi06:27017, vnhcmvt-smartnavi05:27017], passives=[], arbiters=[], primary='vnhcmvt-smartnavi06:27017', tagSet=TagSet{[]}, electionId=null, setVersion=3, topologyVersion=TopologyVersion{processId=643374b88fa0c251735bc194, counter=4}, lastWriteDate=Mon Apr 10 16:01:18 ICT 2023, lastUpdateTimeNanos=540772998495132}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/10 16:01:19 INFO FileOutputCommitter: Saved output of task 'attempt_20230410155805360052142874823818_0012_m_000023_327' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_20230410155805360052142874823818_0012_m_000023\n",
      "23/04/10 16:01:19 INFO SparkHadoopMapRedUtil: attempt_20230410155805360052142874823818_0012_m_000023_327: Committed\n",
      "23/04/10 16:01:19 INFO Executor: Finished task 23.0 in stage 12.0 (TID 327). 2515 bytes result sent to driver\n",
      "23/04/10 16:01:19 INFO TaskSetManager: Finished task 23.0 in stage 12.0 (TID 327) in 97442 ms on localhost (executor driver) (25/36)\n",
      "23/04/10 16:01:26 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558051927188446984604324_0012_m_000019_323' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558051927188446984604324_0012_m_000019\n",
      "23/04/10 16:01:26 INFO SparkHadoopMapRedUtil: attempt_202304101558051927188446984604324_0012_m_000019_323: Committed\n",
      "23/04/10 16:01:26 INFO Executor: Finished task 19.0 in stage 12.0 (TID 323). 2515 bytes result sent to driver\n",
      "23/04/10 16:01:26 INFO TaskSetManager: Finished task 19.0 in stage 12.0 (TID 323) in 116495 ms on localhost (executor driver) (26/36)\n",
      "23/04/10 16:01:39 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558058140059688530557508_0012_m_000026_330' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558058140059688530557508_0012_m_000026\n",
      "23/04/10 16:01:39 INFO SparkHadoopMapRedUtil: attempt_202304101558058140059688530557508_0012_m_000026_330: Committed\n",
      "23/04/10 16:01:39 INFO Executor: Finished task 26.0 in stage 12.0 (TID 330). 2515 bytes result sent to driver\n",
      "23/04/10 16:01:39 INFO TaskSetManager: Finished task 26.0 in stage 12.0 (TID 330) in 81871 ms on localhost (executor driver) (27/36)\n",
      "23/04/10 16:01:41 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558054639960498397502602_0012_m_000021_325' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558054639960498397502602_0012_m_000021\n",
      "23/04/10 16:01:41 INFO SparkHadoopMapRedUtil: attempt_202304101558054639960498397502602_0012_m_000021_325: Committed\n",
      "23/04/10 16:01:41 INFO Executor: Finished task 21.0 in stage 12.0 (TID 325). 2515 bytes result sent to driver\n",
      "23/04/10 16:01:41 INFO TaskSetManager: Finished task 21.0 in stage 12.0 (TID 325) in 127902 ms on localhost (executor driver) (28/36)\n",
      "23/04/10 16:01:45 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558058080549769141201208_0012_m_000030_334' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558058080549769141201208_0012_m_000030\n",
      "23/04/10 16:01:45 INFO SparkHadoopMapRedUtil: attempt_202304101558058080549769141201208_0012_m_000030_334: Committed\n",
      "23/04/10 16:01:45 INFO Executor: Finished task 30.0 in stage 12.0 (TID 334). 2515 bytes result sent to driver\n",
      "23/04/10 16:01:45 INFO TaskSetManager: Finished task 30.0 in stage 12.0 (TID 334) in 48324 ms on localhost (executor driver) (29/36)\n",
      "23/04/10 16:01:52 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558057863554216596217054_0012_m_000035_339' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558057863554216596217054_0012_m_000035\n",
      "23/04/10 16:01:52 INFO SparkHadoopMapRedUtil: attempt_202304101558057863554216596217054_0012_m_000035_339: Committed\n",
      "23/04/10 16:01:52 INFO Executor: Finished task 35.0 in stage 12.0 (TID 339). 2515 bytes result sent to driver\n",
      "23/04/10 16:01:52 INFO TaskSetManager: Finished task 35.0 in stage 12.0 (TID 339) in 34718 ms on localhost (executor driver) (30/36)\n",
      "23/04/10 16:01:55 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558058663830786946952099_0012_m_000028_332' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558058663830786946952099_0012_m_000028\n",
      "23/04/10 16:01:55 INFO SparkHadoopMapRedUtil: attempt_202304101558058663830786946952099_0012_m_000028_332: Committed\n",
      "23/04/10 16:01:55 INFO Executor: Finished task 28.0 in stage 12.0 (TID 332). 2515 bytes result sent to driver\n",
      "23/04/10 16:01:55 INFO TaskSetManager: Finished task 28.0 in stage 12.0 (TID 332) in 70297 ms on localhost (executor driver) (31/36)\n",
      "23/04/10 16:01:56 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558056400813131010134341_0012_m_000031_335' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558056400813131010134341_0012_m_000031\n",
      "23/04/10 16:01:56 INFO SparkHadoopMapRedUtil: attempt_202304101558056400813131010134341_0012_m_000031_335: Committed\n",
      "23/04/10 16:01:56 INFO Executor: Finished task 31.0 in stage 12.0 (TID 335). 2515 bytes result sent to driver\n",
      "23/04/10 16:01:56 INFO TaskSetManager: Finished task 31.0 in stage 12.0 (TID 335) in 55773 ms on localhost (executor driver) (32/36)\n",
      "23/04/10 16:01:57 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558051911966492633349953_0012_m_000034_338' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558051911966492633349953_0012_m_000034\n",
      "23/04/10 16:01:57 INFO SparkHadoopMapRedUtil: attempt_202304101558051911966492633349953_0012_m_000034_338: Committed\n",
      "23/04/10 16:01:57 INFO Executor: Finished task 34.0 in stage 12.0 (TID 338). 2515 bytes result sent to driver\n",
      "23/04/10 16:01:57 INFO TaskSetManager: Finished task 34.0 in stage 12.0 (TID 338) in 40303 ms on localhost (executor driver) (33/36)\n",
      "23/04/10 16:02:01 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558053359958515019680674_0012_m_000029_333' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558053359958515019680674_0012_m_000029\n",
      "23/04/10 16:02:01 INFO SparkHadoopMapRedUtil: attempt_202304101558053359958515019680674_0012_m_000029_333: Committed\n",
      "23/04/10 16:02:01 INFO Executor: Finished task 29.0 in stage 12.0 (TID 333). 2515 bytes result sent to driver\n",
      "23/04/10 16:02:01 INFO TaskSetManager: Finished task 29.0 in stage 12.0 (TID 333) in 67086 ms on localhost (executor driver) (34/36)\n",
      "23/04/10 16:02:01 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558051715540536298288896_0012_m_000032_336' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558051715540536298288896_0012_m_000032\n",
      "23/04/10 16:02:01 INFO SparkHadoopMapRedUtil: attempt_202304101558051715540536298288896_0012_m_000032_336: Committed\n",
      "23/04/10 16:02:01 INFO Executor: Finished task 32.0 in stage 12.0 (TID 336). 2515 bytes result sent to driver\n",
      "23/04/10 16:02:01 INFO TaskSetManager: Finished task 32.0 in stage 12.0 (TID 336) in 60686 ms on localhost (executor driver) (35/36)\n",
      "23/04/10 16:02:03 INFO FileOutputCommitter: Saved output of task 'attempt_202304101558058189264007393612613_0012_m_000033_337' to file:/home/spark/ylv/data/navidata_csv_1/navidata.csv/_temporary/0/task_202304101558058189264007393612613_0012_m_000033\n",
      "23/04/10 16:02:03 INFO SparkHadoopMapRedUtil: attempt_202304101558058189264007393612613_0012_m_000033_337: Committed\n",
      "23/04/10 16:02:03 INFO Executor: Finished task 33.0 in stage 12.0 (TID 337). 2515 bytes result sent to driver\n",
      "23/04/10 16:02:03 INFO TaskSetManager: Finished task 33.0 in stage 12.0 (TID 337) in 48570 ms on localhost (executor driver) (36/36)\n",
      "23/04/10 16:02:03 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "23/04/10 16:02:03 INFO DAGScheduler: ResultStage 12 (save at NativeMethodAccessorImpl.java:0) finished in 238.622 s\n",
      "23/04/10 16:02:03 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/04/10 16:02:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
      "23/04/10 16:02:03 INFO DAGScheduler: Job 10 finished: save at NativeMethodAccessorImpl.java:0, took 238.623169 s\n",
      "23/04/10 16:02:03 INFO FileFormatWriter: Start to commit write Job 82e3ed9f-91eb-4339-a2c6-9a682030bcaf.\n",
      "23/04/10 16:02:03 INFO FileFormatWriter: Write Job 82e3ed9f-91eb-4339-a2c6-9a682030bcaf committed. Elapsed time: 13 ms.\n",
      "23/04/10 16:02:03 INFO FileFormatWriter: Finished processing stats for write job 82e3ed9f-91eb-4339-a2c6-9a682030bcaf.\n"
     ]
    }
   ],
   "source": [
    "df.write.format(\"csv\").save(\"/home/spark/ylv/data/navidata_csv_1/navidata.csv\", schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b96f7b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.mongodb.write.connection.uri',\n",
       "  'mongodb://10.122.6.17:27017/vnw_job.events=None'),\n",
       " ('spark.jars',\n",
       "  '/home/spark/Downloads/rapids-4-spark_2.12-23.02.0.jar,/home/spark/spark/jars/mongo-spark-connector/bson-4.8.2.jar,/home/spark/spark/jars/mongo-spark-connector/bson-record-codec-4.8.2.jar,/home/spark/spark/jars/mongo-spark-connector/mongodb-driver-core-4.8.2.jar,/home/spark/spark/jars/mongo-spark-connector/mongodb-driver-sync-4.8.2.jar,/home/spark/spark/jars/mongo-spark-connector/mongo-spark-connector_2.12-10.1.1.jar,/home/spark/Downloads/spark-tensorflow-connector.jar'),\n",
       " ('spark.app.name', 'myApp'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///home/spark/Downloads/rapids-4-spark_2.12-23.02.0.jar,file:///home/spark/spark/jars/mongo-spark-connector/bson-4.8.2.jar,file:///home/spark/spark/jars/mongo-spark-connector/bson-record-codec-4.8.2.jar,file:///home/spark/spark/jars/mongo-spark-connector/mongodb-driver-core-4.8.2.jar,file:///home/spark/spark/jars/mongo-spark-connector/mongodb-driver-sync-4.8.2.jar,file:///home/spark/spark/jars/mongo-spark-connector/mongo-spark-connector_2.12-10.1.1.jar,file:/home/spark/Downloads/spark-tensorflow-connector.jar'),\n",
       " ('spark.driver.port', '32921'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.mongodb.read.connection.uri',\n",
       "  'mongodb://10.122.6.17:27017/vnw_job.events=None'),\n",
       " ('spark.driver.host', 'localhost'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://localhost:32921/jars/rapids-4-spark_2.12-23.02.0.jar,spark://localhost:32921/jars/bson-4.8.2.jar,spark://localhost:32921/jars/mongodb-driver-sync-4.8.2.jar,spark://localhost:32921/jars/mongo-spark-connector_2.12-10.1.1.jar,spark://localhost:32921/jars/bson-record-codec-4.8.2.jar,spark://localhost:32921/jars/mongodb-driver-core-4.8.2.jar'),\n",
       " ('spark.sql.warehouse.dir', 'file:/home/spark/ylv/workplace/spark-warehouse'),\n",
       " ('spark.master', 'local[12]'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.startTime', '1681110287594'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.mongodb.read.connection.uri=mongodb://10.122.6.17:27017/vnw_job.events',\n",
       "  'None'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.id', 'local-1681110288132'),\n",
       " ('spark.mongodb.write.connection.uri=mongodb://10.122.6.17:27017/vnw_job.events',\n",
       "  'None')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f02fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-01-07T00:00:00Z'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "datetime.strftime(datetime.now(), '%Y-01-%mT00:00:00Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c949bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
